{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: baseline\n",
      "Running experiment: unexpected_reward\n",
      "Running experiment: reward_omission\n",
      "All experiments completed successfully!\n",
      "Results saved to: /Users/david/Desktop/rl_dopamine_demo\n"
     ]
    }
   ],
   "source": [
    "import os, json, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\"GridWorld environment with configurable parameters\"\"\"\n",
    "    \n",
    "    def __init__(self, width: int = 6, height: int = 6, start: Tuple[int, int] = (0,0), \n",
    "                 goal: Tuple[int, int] = (5,5), slip: float = 0.05, step_penalty: float = -0.01):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.start = start\n",
    "        self.agent = list(start)\n",
    "        self.goal = goal\n",
    "        self.slip = slip\n",
    "        self.step_penalty = step_penalty\n",
    "        self.n_states = width * height\n",
    "        self.action_space = [(0,1), (0,-1), (1,0), (-1,0)]  # right, left, down, up\n",
    "        self.n_actions = len(self.action_space)\n",
    "        \n",
    "        # Validate inputs\n",
    "        self._validate_positions()\n",
    "\n",
    "    def _validate_positions(self):\n",
    "        \"\"\"Validate start and goal positions\"\"\"\n",
    "        for pos, name in [(self.start, \"start\"), (self.goal, \"goal\")]:\n",
    "            if not (0 <= pos[0] < self.width and 0 <= pos[1] < self.height):\n",
    "                raise ValueError(f\"{name} position {pos} is outside grid bounds\")\n",
    "\n",
    "    def reset(self) -> int:\n",
    "        \"\"\"Reset environment and return initial state index\"\"\"\n",
    "        self.agent = list(self.start)\n",
    "        return self._state_index(tuple(self.agent))\n",
    "\n",
    "    def _state_index(self, pos: Tuple[int, int]) -> int:\n",
    "        \"\"\"Convert position to state index\"\"\"\n",
    "        x, y = pos\n",
    "        return y * self.width + x\n",
    "\n",
    "    def _pos_from_index(self, idx: int) -> Tuple[int, int]:\n",
    "        \"\"\"Convert state index to position\"\"\"\n",
    "        x = idx % self.width\n",
    "        y = idx // self.width\n",
    "        return (x, y)\n",
    "\n",
    "    def step(self, action: int, extra_reward: float = 0.0, omit_reward: bool = False) -> Tuple[int, float, bool, dict]:\n",
    "        \"\"\"Execute one environment step\"\"\"\n",
    "        # Random slip\n",
    "        if np.random.rand() < self.slip:\n",
    "            action = np.random.randint(self.n_actions)\n",
    "            \n",
    "        dx, dy = self.action_space[action]\n",
    "        new_x = np.clip(self.agent[0] + dx, 0, self.width-1)\n",
    "        new_y = np.clip(self.agent[1] + dy, 0, self.height-1)\n",
    "        self.agent = [new_x, new_y]\n",
    "        \n",
    "        done = tuple(self.agent) == self.goal\n",
    "        \n",
    "        # Calculate reward\n",
    "        if done:\n",
    "            reward = 0.0 if omit_reward else (1.0 + extra_reward)\n",
    "        else:\n",
    "            reward = self.step_penalty\n",
    "            \n",
    "        return self._state_index(tuple(self.agent)), reward, done, {}\n",
    "\n",
    "    def render_policy(self, policy: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Render policy as grid of arrows\"\"\"\n",
    "        grid = np.full((self.height, self.width), ' ', dtype=object)\n",
    "        arrows = {0: '→', 1: '←', 2: '↓', 3: '↑'}\n",
    "        \n",
    "        for s in range(self.n_states):\n",
    "            pos = self._pos_from_index(s)\n",
    "            if pos == self.goal:\n",
    "                grid[pos[1], pos[0]] = 'G'\n",
    "            elif pos == tuple(self.start):\n",
    "                grid[pos[1], pos[0]] = 'S'\n",
    "            else:\n",
    "                grid[pos[1], pos[0]] = arrows[policy[s]]\n",
    "                \n",
    "        return grid\n",
    "\n",
    "    def visualize_grid(self, ax=None):\n",
    "        \"\"\"Visualize the grid world\"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(8, 8))\n",
    "            \n",
    "        # Create grid\n",
    "        for x in range(self.width + 1):\n",
    "            ax.axvline(x, color='black', linewidth=1)\n",
    "        for y in range(self.height + 1):\n",
    "            ax.axhline(y, color='black', linewidth=1)\n",
    "            \n",
    "        # Mark start and goal\n",
    "        start_x, start_y = self.start\n",
    "        goal_x, goal_y = self.goal\n",
    "        \n",
    "        ax.add_patch(plt.Rectangle((start_x, start_y), 1, 1, fill=True, color='green', alpha=0.3))\n",
    "        ax.add_patch(plt.Rectangle((goal_x, goal_y), 1, 1, fill=True, color='red', alpha=0.3))\n",
    "        \n",
    "        ax.text(start_x + 0.5, start_y + 0.5, 'S', ha='center', va='center', fontsize=20, fontweight='bold')\n",
    "        ax.text(goal_x + 0.5, goal_y + 0.5, 'G', ha='center', va='center', fontsize=20, fontweight='bold')\n",
    "        \n",
    "        ax.set_xlim(0, self.width)\n",
    "        ax.set_ylim(0, self.height)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_title('GridWorld Environment')\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        \n",
    "        return ax\n",
    "\n",
    "\n",
    "class ActorCritic:\n",
    "    \"\"\"Actor-Critic agent with softmax policy\"\"\"\n",
    "    \n",
    "    def __init__(self, n_states: int, n_actions: int, alpha_v: float = 0.2, \n",
    "                 alpha_pi: float = 0.05, gamma: float = 0.99):\n",
    "        self.V = np.zeros(n_states)  # State-value function\n",
    "        self.preferences = np.zeros((n_states, n_actions))  # Action preferences\n",
    "        self.alpha_v = alpha_v  # Learning rate for value function\n",
    "        self.alpha_pi = alpha_pi  # Learning rate for policy\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def policy(self, state: int) -> np.ndarray:\n",
    "        \"\"\"Compute softmax policy probabilities for a state\"\"\"\n",
    "        prefs = self.preferences[state]\n",
    "        # Numerical stability: subtract max before exp\n",
    "        exp_p = np.exp(prefs - np.max(prefs))\n",
    "        return exp_p / np.sum(exp_p)\n",
    "\n",
    "    def select_action(self, state: int) -> int:\n",
    "        \"\"\"Select action according to current policy\"\"\"\n",
    "        probs = self.policy(state)\n",
    "        return np.random.choice(self.n_actions, p=probs)\n",
    "\n",
    "    def update(self, state: int, action: int, reward: float, next_state: int, done: bool) -> float:\n",
    "        \"\"\"Update value function and policy using TD error\"\"\"\n",
    "        # Calculate TD target and error\n",
    "        target = reward + (0 if done else self.gamma * self.V[next_state])\n",
    "        delta = target - self.V[state]  # TD error (dopamine-like signal)\n",
    "        \n",
    "        # Update value function\n",
    "        self.V[state] += self.alpha_v * delta\n",
    "        \n",
    "        # Update policy using policy gradient\n",
    "        probs = self.policy(state)\n",
    "        for a in range(self.n_actions):\n",
    "            # Gradient of log policy\n",
    "            grad_log = (1 if a == action else 0) - probs[a]\n",
    "            self.preferences[state, a] += self.alpha_pi * delta * grad_log\n",
    "            \n",
    "        return delta\n",
    "\n",
    "\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for experiments\"\"\"\n",
    "    \n",
    "    def __init__(self, total_episodes: int = 300, max_steps: int = 150, \n",
    "                 window_size: int = 30, smoothing_window: int = 25):\n",
    "        self.total_episodes = total_episodes\n",
    "        self.max_steps = max_steps\n",
    "        self.window_size = window_size\n",
    "        self.smoothing_window = smoothing_window\n",
    "        \n",
    "        # Experiment timing (middle 10% of episodes)\n",
    "        self.start_episode = total_episodes // 2\n",
    "        self.end_episode = self.start_episode + max(1, total_episodes // 10)\n",
    "\n",
    "\n",
    "def train_with_events(env: GridWorld, agent: ActorCritic, config: ExperimentConfig, \n",
    "                     experiment: Optional[str] = None) -> Dict:\n",
    "    \"\"\"Train agent and record events\"\"\"\n",
    "    returns = []\n",
    "    td_errors = []\n",
    "    events = []\n",
    "    global_step = 0\n",
    "    \n",
    "    for episode in range(config.total_episodes):\n",
    "        state = env.reset()\n",
    "        episode_return = 0.0\n",
    "        \n",
    "        for step in range(config.max_steps):\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            # Apply experimental manipulations\n",
    "            extra_reward = 0.0\n",
    "            omit_reward = False\n",
    "            \n",
    "            if experiment == \"unexpected_reward\" and config.start_episode <= episode < config.end_episode:\n",
    "                extra_reward = 2.0\n",
    "            elif experiment == \"reward_omission\" and config.start_episode <= episode < config.end_episode:\n",
    "                omit_reward = True\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action, extra_reward=extra_reward, omit_reward=omit_reward)\n",
    "            td_error = agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            td_errors.append(td_error)\n",
    "            \n",
    "            # Record event if episode ended\n",
    "            if done:\n",
    "                event_type = \"normal_reward\"\n",
    "                if experiment == \"unexpected_reward\" and config.start_episode <= episode < config.end_episode:\n",
    "                    event_type = \"unexpected_reward\"\n",
    "                elif experiment == \"reward_omission\" and config.start_episode <= episode < config.end_episode:\n",
    "                    event_type = \"reward_omission\"\n",
    "                    \n",
    "                events.append((global_step, event_type, episode, step))\n",
    "            \n",
    "            episode_return += reward\n",
    "            state = next_state\n",
    "            global_step += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        returns.append(episode_return)\n",
    "    \n",
    "    return {\n",
    "        \"returns\": returns,\n",
    "        \"td_errors\": td_errors,\n",
    "        \"events\": events,\n",
    "        \"agent\": agent\n",
    "    }\n",
    "\n",
    "\n",
    "def align_traces(td_errors: List[float], events: List[tuple], window_size: int = 30) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Align TD error traces around events\"\"\"\n",
    "    traces = {}\n",
    "    \n",
    "    for step_idx, event_type, ep, t in events:\n",
    "        start_idx = max(0, step_idx - window_size)\n",
    "        end_idx = min(len(td_errors), step_idx + window_size + 1)\n",
    "        trace = td_errors[start_idx:end_idx]\n",
    "        \n",
    "        # Pad with NaN if necessary\n",
    "        if len(trace) < 2 * window_size + 1:\n",
    "            left_pad = window_size - (step_idx - start_idx)\n",
    "            right_pad = (2 * window_size + 1) - len(trace) - left_pad\n",
    "            trace = [np.nan] * left_pad + trace + [np.nan] * right_pad\n",
    "            \n",
    "        traces.setdefault(event_type, []).append(trace)\n",
    "    \n",
    "    # Compute mean traces\n",
    "    traces_mean = {}\n",
    "    for event_type, event_traces in traces.items():\n",
    "        arr = np.array(event_traces, dtype=float)\n",
    "        traces_mean[event_type] = np.nanmean(arr, axis=0)\n",
    "        \n",
    "    return traces_mean\n",
    "\n",
    "\n",
    "def plot_results(experiment_results: Dict, config: ExperimentConfig, output_dir: str):\n",
    "    \"\"\"Create comprehensive plots for all experiments\"\"\"\n",
    "    \n",
    "    # 1. Plot returns for all experiments\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    for i, (exp_name, results) in enumerate(experiment_results.items()):\n",
    "        returns = results[\"returns\"]\n",
    "        \n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.plot(returns, alpha=0.7, label=\"Raw returns\")\n",
    "        \n",
    "        if len(returns) > config.smoothing_window:\n",
    "            smoothed = np.convolve(returns, np.ones(config.smoothing_window)/config.smoothing_window, \n",
    "                                 mode='valid')\n",
    "            plt.plot(range(config.smoothing_window-1, len(returns)), smoothed, \n",
    "                    label=f\"Smoothed\", linewidth=2)\n",
    "        \n",
    "        # Mark experiment period\n",
    "        if exp_name != \"baseline\":\n",
    "            plt.axvspan(config.start_episode, config.end_episode, \n",
    "                       alpha=0.2, color='red', label='Manipulation')\n",
    "        \n",
    "        plt.title(f\"{exp_name.replace('_', ' ').title()}\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Return\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"all_returns_comparison.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Plot aligned TD errors\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i, (exp_name, results) in enumerate(experiment_results.items()):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        \n",
    "        traces_mean = align_traces(results[\"td_errors\"], results[\"events\"], config.window_size)\n",
    "        \n",
    "        for event_type, mean_trace in traces_mean.items():\n",
    "            x = np.arange(-config.window_size, config.window_size + 1)\n",
    "            plt.plot(x, mean_trace, label=event_type.replace('_', ' ').title(), linewidth=2)\n",
    "        \n",
    "        plt.axvline(0, linestyle='--', color='red', alpha=0.7, label='Event')\n",
    "        plt.axhline(0, linestyle='-', color='black', alpha=0.5)\n",
    "        \n",
    "        plt.title(f\"TD Errors: {exp_name.replace('_', ' ').title()}\")\n",
    "        plt.xlabel(\"Steps Relative to Goal\")\n",
    "        plt.ylabel(\"Mean TD Error\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"all_td_errors_comparison.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Set random seeds for reproducibility\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Configuration\n",
    "    output_dir = \"/Users/david/Desktop/rl_dopamine_demo\"\n",
    "    config = ExperimentConfig()\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Run experiments\n",
    "    experiment_results = {}\n",
    "    \n",
    "    for exp_name in [\"baseline\", \"unexpected_reward\", \"reward_omission\"]:\n",
    "        print(f\"Running experiment: {exp_name}\")\n",
    "        \n",
    "        env = GridWorld(width=6, height=6, start=(0,0), goal=(5,5), slip=0.05)\n",
    "        agent = ActorCritic(env.n_states, env.n_actions)\n",
    "        \n",
    "        results = train_with_events(env, agent, config, \n",
    "                                  experiment=exp_name if exp_name != \"baseline\" else None)\n",
    "        experiment_results[exp_name] = results\n",
    "    \n",
    "    # Create comprehensive plots\n",
    "    plot_results(experiment_results, config, output_dir)\n",
    "    \n",
    "    # Save learned policies\n",
    "    env_template = GridWorld(width=6, height=6, start=(0,0), goal=(5,5))\n",
    "    \n",
    "    for exp_name, results in experiment_results.items():\n",
    "        policy = results[\"agent\"].preferences.argmax(axis=1)\n",
    "        policy_grid = env_template.render_policy(policy)\n",
    "        \n",
    "        filename = os.path.join(output_dir, f\"learned_policy_{exp_name}.txt\")\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(f\"Learned Policy - {exp_name}\\n\")\n",
    "            f.write(\"=\" * 40 + \"\\n\")\n",
    "            for row in policy_grid:\n",
    "                f.write(\" \".join(row) + \"\\n\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"experiments\": list(experiment_results.keys()),\n",
    "        \"config\": {\n",
    "            \"total_episodes\": config.total_episodes,\n",
    "            \"max_steps\": config.max_steps,\n",
    "            \"window_size\": config.window_size,\n",
    "            \"manipulation_episodes\": f\"{config.start_episode}-{config.end_episode}\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    # Create environment visualization\n",
    "    env_viz = GridWorld()\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    env_viz.visualize_grid(ax)\n",
    "    plt.savefig(os.path.join(output_dir, \"gridworld_environment.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"All experiments completed successfully!\")\n",
    "    print(f\"Results saved to: {output_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
